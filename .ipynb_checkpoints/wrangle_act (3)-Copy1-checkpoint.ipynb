{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction :\n",
    "\n",
    "WeRateDogs Twitter data to wrangling ,analyses ,visualizing And exploratary anaylsis .The data collected was from the Twitter account.People's rate dogs with a comment and Analysis done on the basis of ratings almost always have a denominator of 10 and image prediction,tweets ,retweets and favorite .\n",
    "\n",
    "1.Data wrangling, which consists of:\n",
    "-Gathering data .\n",
    "-Assessing data.\n",
    "-Cleaning data.\n",
    "Storing, analyzing, and visualizing WeRateDogs wrangled data and finally combined data in master document to be used in data analysis and visualization.\n",
    "\n",
    "2.Analysis is done on the basis of rating, tweets and retweet.\n",
    "\n",
    "-Analyse the relationship between ratings, favourite counts and retweets_count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import tweepy\n",
    "import os\n",
    "import requests\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from timeit import default_timer as timer\n",
    "from lxml import html \n",
    "import re\n",
    "import matplotlib\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline\n",
    "matplotlib.font_manager._rebuild()\n",
    "sns.set(style=\"darkgrid\",font_scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data Source 1\n",
    "### Twitter-Archive-enhanced Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter-archive-enhanced .csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ### Gathering Data Source 2\n",
    "\n",
    "### Image-predictions Data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = requests.get(url)\n",
    "f = open('image-predictions.tsv', \"w\")\n",
    "f.write(response.text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tsv_read = pd.read_csv('image-predictions.tsv' ,sep='\\t')\n",
    "tsv_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data Source3\n",
    "\n",
    "## Programmatically download the json files from the Twitter Account .\n",
    "\n",
    "Programmatically download the json files from the Twitter Account .\n",
    "Query the Twitter API for each tweet's JSON data using Python's Tweepy library -Store each tweet's entire set of JSON data in a file called tweet_json.txt file. -Each tweet's JSON data should be written and Then read this .txt file line by line into a pandas DataFrame with tweet ID, retweet count,and favorite count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping tweets from twitter took a lot of time. The folling commented code was run once and scraped results were stored in a file for subsequent reads.\n",
    "\n",
    "#consumer_key = '****'\n",
    "#consumer_secret = '****'\n",
    "#access_token = '****'\n",
    "#access_secret = '****'\n",
    "\n",
    "#auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "#auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "#api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "#api\n",
    "#tweet_data = {}\n",
    "#fails_dict = {}\n",
    "#start = timer()\n",
    "#for tweet_id in tweet_ids:\n",
    " #   try:\n",
    "  #      tweet_status = api.get_status(tweet_id, tweet_mode='extended')         \n",
    "   #     tweet_data[str(tweet_id)] = tweet_status._json\n",
    "    #    print(tweet_data[str(tweet_id)])\n",
    "    #except tweepy.TweepError as e:\n",
    "     #   print(e)\n",
    "      #  print(\"Error for: \" +str(tweet_id))\n",
    "       # fails_dict[str(tweet_id)] = e\n",
    "\n",
    "#end = timer()\n",
    "#print(end - start)\n",
    "#print(fails_dict)\n",
    "#print(\"Length : %d\"% len (fails_dict))\n",
    "#with open('tweet_json.txt', 'w') as file:\n",
    "#    json.dump(tweet_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_ids = list(df.tweet_id)\n",
    "tweet_df = pd.DataFrame(columns=['tweet_id', 'retweet_count','favorite_count'])\n",
    "\n",
    "# Read text file line by line into a data frame\n",
    "with open('tweet-json.txt') as f:\n",
    "    for line in f:\n",
    "        status = json.loads(line)\n",
    "        tweet_id = status['id_str']\n",
    "        retweet_count = status['retweet_count']\n",
    "        favorite_count = status['favorite_count']\n",
    "        \n",
    "        tweet_df = tweet_df.append(pd.DataFrame([[tweet_id, retweet_count, favorite_count]], \n",
    "                                       columns = ['tweet_id', 'retweet_count', 'favorite_count']))\n",
    "tweet_df = tweet_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = pd.Series(list(df) + list(tsv_read) + list(tweet_df))\n",
    "all_columns[all_columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Data:\n",
    "- Assess the individual dataset performing the following method:.head(),.tail(),.info(),.describe(),.nunique(),.isnull().,.sample(),.duplicated().sum()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1: Twitter-Archive-enhanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "1. in_reply_to_status_id only 78 non null values.\n",
    "\n",
    "2. in_reply_to_user_id data only 78 non null values.\n",
    "\n",
    "3. timestamp data type is in string format .\n",
    "\n",
    "4. retweeted_status_id  only 181 non null values.\n",
    "\n",
    "5. retweeted_status_user_id only 181 id retweeted\n",
    "\n",
    "6. retweeted_status_timestamp datatype is in string format .\n",
    "\n",
    "7. expanded_urls data missing.total url: 2297 non null values\n",
    "\n",
    "8. source field contains url inside html statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2356 tweets\n",
    "* 957 different dogs referenced\n",
    "* 4 different sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2: Image Predictions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_read.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_read.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations: Tweet image predictions columns:\n",
    "\n",
    "1. tweet_id: the unique identifier for each tweet\n",
    "\n",
    "2. jpg_url: dog's image URL\n",
    "\n",
    "3. img_num: the image number that corresponded to the most confident prediction.\n",
    "\n",
    "4. p1 is the algorithm's #1 prediction for the image in the tweet \n",
    "\n",
    "\n",
    "5. p1_conf: how confident the algorithm is in its #1 prediction\n",
    "\n",
    "6. p2 is the algorithm's second most likely prediction \n",
    "\n",
    "7. p2_conf: how confident the algorithm is in its #2 prediction\n",
    "\n",
    "8. p3 is the algorithm's second most likely prediction \n",
    "\n",
    "9. p3_conf: how confident the algorithm is in its #3 prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsv_read.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total tweet_id for image prediction :2075 and analysis of twitter archived data is 2356 so, there is missing image data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_read.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_read.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 3: Tweepy to access Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweet_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    " - Coloumns present in the dataset : created_at, id, , id_str, full_text, truncated, display_text_range, entities, extended_entities, source, in_reply_to_status_id, in_reply_to_status_id_str',in_reply_to_user_id, in_reply_to_user_id_str,in_reply_to_screen_name, geo, coordinates,place, contributors, is_quote_status, retweet_count,favorite_count,favorited,retweeted, possibly_sensitive, possibly_sensitive_appealable, lang\n",
    "\n",
    "     *All the fields are self explainotry. \n",
    "\n",
    "Here we consider on retweet and favorite counts for wrangling and analysis as per the problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling  :\n",
    "\n",
    "#### Cleaning Issues:\n",
    "\n",
    "* Low quality data has content issues.\n",
    "\n",
    "* Untidy data has structural issues\n",
    "\n",
    "#### Cleaning Issues Twitter-Archive-enhanced Data:\n",
    "\n",
    "    1.timestamp data type is in string format .\n",
    "\n",
    "    2.Datatype error.\n",
    "\n",
    "    3.extract url from source html tag\n",
    "    \n",
    "    4.Remove White space from tweets.\n",
    "    \n",
    "    5.Invalid names\n",
    "    \n",
    "#### Cleaning Issues in Tweepy to access Twitter data\n",
    "    6. Datatype error: String Value datatype tweet_id,retweet_count,favorite_count convert into int64\n",
    "    \n",
    "####  Cleaning Issues detect in df_clean :\n",
    "    7.Detect missing values\n",
    "    \n",
    "    -in_reply_to_status_id\n",
    "    -in_reply_to_user_id\n",
    "    -retweeted_status_id\n",
    "    -retweeted_status_user_id\n",
    "    -retweeted_status_timestamp\n",
    "    -expanded_urls.\n",
    "    \n",
    "    8.Datatype issue:Convert in_reply_to_status_id and in_reply_to_user_id to data type integer.\n",
    "    \n",
    "    9.Drop_duplicates from merge_data.\n",
    "    \n",
    "    10.Detect missing value Nan issue can fixed using fillna(0)from merge data1.\n",
    "    \n",
    "    11.Drop duplicates( tweet_id) from merge data1.  \n",
    "    \n",
    "#### Tidiness issues 1 \n",
    "\n",
    "    -Melt (Combine) the four dog stages 'doggo', 'floofer', 'pupper', and 'puppo' into one column          dog_stage.\n",
    "\n",
    "#### Tidiness issues 2:\n",
    "\n",
    "    -Merge two dataframe merge_data,tweet_df_clean that contain information about rating and tweets .\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Issues Twitter-Archive-enhanced Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning Issues 1\n",
    "\n",
    "   Timestamp data type is in string format convert timestamp data into datetime64\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning Issues 2\n",
    "*Define  \n",
    "retweeted_status_timestamp datatype is in string format convert retweeted_status_timestamp datatype into datetime64.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Code\n",
    "df['retweeted_status_timestamp'] = pd.to_datetime(df['retweeted_status_timestamp'])\n",
    "df['retweeted_status_timestamp'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Issues 3\n",
    "\n",
    "**Define\n",
    "\n",
    "Source show ahref (ahref=\"http://twitter.com/download/iphone\" r.._) Remove unnecessary tags from source column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUrl(htmlString):\n",
    "    return re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', htmlString)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['source'] = df['source'].apply(getUrl)\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning issues 4\n",
    "**Define\n",
    "Remove White space from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code\n",
    "df['text'] = df['text'].replace('\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Issues 5\n",
    "**Define\n",
    "- Invalid names  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Code\n",
    "names= df['name'].str.contains('^[a-z]', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[names].name.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted dog name contain invalid name such as :a,an ,the,very, etc. Such dog tweets may be dropped as such but I chose to keep the tweets. Whats in the name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweepy to access Twitter data\n",
    "\n",
    "###### Cleaning issues 6\n",
    "  - Define : Datatype error:String Value datatype tweet_id,retweet_count,favorite_count convert into int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code \n",
    "tweet_df['tweet_id'] = tweet_df['tweet_id'].astype('int64')\n",
    "tweet_df['retweet_count'] = tweet_df['retweet_count'].astype('int64')\n",
    "tweet_df['favorite_count'] = tweet_df['favorite_count'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "tweet_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy of all the dataframes\n",
    "\n",
    "***Define : Create the copy of all the dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()\n",
    "tsv_read_clean = tsv_read.copy()\n",
    "tweet_df_clean = tweet_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "df_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_read_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues 7\n",
    "***Define\n",
    "#### Detect missing values: in\n",
    "-in_reply_to_status_id\n",
    "-in_reply_to_user_id\n",
    "-retweeted_status_id\n",
    "-retweeted_status_user_id\n",
    "-'retweeted_status_timestamp'\n",
    "-'expanded_urls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['in_reply_to_status_id']=df_clean['in_reply_to_status_id'].fillna(0)\n",
    "df_clean['in_reply_to_user_id']=df_clean['in_reply_to_user_id'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean['retweeted_status_id']=df_clean['retweeted_status_id'].fillna(0)\n",
    "df_clean['retweeted_status_user_id']=df_clean['retweeted_status_user_id'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['retweeted_status_timestamp']=df_clean['retweeted_status_timestamp'].fillna(0)\n",
    "df_clean['expanded_urls']=df_clean['expanded_urls'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix Datatype issue 8:\n",
    "\n",
    "***Define :\n",
    "\n",
    "Convert in_reply_to_status_id and in_reply_to_user_id to data type integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['in_reply_to_status_id'] = df_clean['in_reply_to_status_id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean['in_reply_to_user_id'] = df_clean['in_reply_to_user_id'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean['name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    *Looking at the data set we can see there are random data names in the tweet data base.  Which can be manually removed. But I have chosen to keep the tweets as we just need the retweet and favorite count from the tweet. Name of the dogs dosent matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean['rating_numerator'].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_clean.rating_numerator >= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28 tweets with numerator >= 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean['rating_denominator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean[df_clean['rating_denominator'] <= 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[df_clean['rating_denominator'] == 10].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean['name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 745 counts with the dog name as \"None\".\n",
    "-  55 counts with the dog name as \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean['name'].nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidiness issues 1 \n",
    " \n",
    " Melt (Combine) the four dog stages 'doggo', 'floofer', 'pupper', and 'puppo' into one column dog_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt1 = df_clean[['tweet_id','rating_numerator','rating_denominator','name', 'doggo', 'floofer', 'pupper','puppo']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt1=melt1.melt(id_vars=['tweet_id','rating_numerator','rating_denominator','name'],value_name='dog_stage')\n",
    "melt1= melt1.drop('variable', axis=1)\n",
    "melt1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt1[(melt1.dog_stage =='puppo')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "melt1[melt1['dog_stage'].astype(str).ne('None')].dropna()\n",
    "melt1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "melt1['rating'] = melt1.rating_numerator/df.rating_denominator\n",
    "melt1['rating'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogTypes = melt1['dog_stage']\n",
    "dogTypes\n",
    "melt1[melt1['dog_stage']==\"doggo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Not used as a dog can belog to multiple stages\n",
    "def setNone(dog_stage):\n",
    "    if dog_stage == \"0\":\n",
    "      return \"None\"\n",
    "    else:\n",
    "      return dog_stage\n",
    "\n",
    "melt1['dog_stage'] = melt1.apply(lambda row: setNone(row['dog_stage']), axis=1)\n",
    "\n",
    "ra_filtered = melt1[melt1['name'] != 'None']\n",
    "rt =ra_filtered.sort_values(by=['rating'], ascending=False).drop_duplicates('name',keep='first')\n",
    "rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ra_filtered[(ra_filtered.dog_stage!='None') & ra_filtered.dog_stage!='None'].groupby('dog_stage')['rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_filtered[(ra_filtered.dog_stage =='puppo')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of dogs found in stages of doggo and name called by a and maximun number of dog have called by undefined named like none in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(melt1.name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "melt1[melt1.name != 'None'].name.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "melt1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Store df_clean (Twitter archived data clean )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('df_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsv_read_clean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tsv_read_clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "melt2=tsv_read_clean.melt(id_vars=['tweet_id','p1_conf','p2_conf','p3_conf','jpg_url','p1','p2','p3'],value_name='p_dog')\n",
    "melt2= melt2.drop('variable', axis=1)\n",
    "melt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Store  tsv_read_clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_read_clean.to_csv('tsv_read_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Merge Two Data frame:melt1 and melt2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merge_data= pd.merge(melt1,melt2,\n",
    "                        how = 'inner', on = 'tweet_id')\n",
    "merge_data.head()\n",
    "merge_data[merge_data['dog_stage']==\"doggo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_clean.drop_duplicates(inplace=True)\n",
    "print(tweet_df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues 9\n",
    "***Drop_duplicates from merge_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "merge_data.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merge_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Define:\n",
    "\n",
    "### Tidiness issues 2: \n",
    "    \n",
    "Merge two dataframe merge_data,tweet_df_clean that contain information about rating and tweets .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data1= pd.merge(merge_data,tweet_df_clean,\n",
    "                         how = 'inner', on = 'tweet_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ### Issues 10 :\n",
    "\n",
    "***Define:\n",
    "\n",
    "Detect missing value Nan issue can fixed using fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data1['rating'].fillna(0,inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues 11.\n",
    "***Define: Drop duplicates( tweet_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Code and *Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data1.mask(merge_data1.astype(object).eq('None')).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.source.value_counts().plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mostly everyone used twitter mobile app for posting dog tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anaysis on basis of dog_stage ,name and their rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_filtered[(ra_filtered.dog_stage!='None') & ra_filtered.dog_stage!='None'].groupby('dog_stage')['rating'].mean().plot.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "melt1Copy = melt1.copy();\n",
    "melt1Copy.drop(melt1Copy[melt1Copy['name'] == 'None'].index, inplace = True) \n",
    "melt1Copy[melt1Copy['name'].str.contains('^[A-Z]', regex = True)].groupby('name').count()['rating'].sort_values(ascending=False).nlargest(15).plot(kind='bar');\n",
    "\n",
    "plt.title(\"Dogs Rating\",fontsize=10)\n",
    "plt.xlabel(\"Name\")\n",
    "plt.ylabel(\"Rating\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of retweet and Favorite Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Define:\n",
    "Drop retweet_count and favorite_count null value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_df_clean.retweet_count.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweet_df_clean.favorite_count.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(121)\n",
    "sns.distplot(tweet_df['retweet_count'], color = 'purple')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(121)\n",
    "sns.distplot(tweet_df['favorite_count'], color = 'purple')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if Retweets and Favorites are corelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = sns.lmplot(\"retweet_count\", \"favorite_count\",tweet_df_clean,\n",
    "              scatter_kws = {'marker':'o','color':'brown'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see strong corelation between favorite count and retweet count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "title = fig.suptitle(\"Algorithm Output\", fontsize=14)\n",
    "fig.subplots_adjust(top=0.93, wspace=0.3)\n",
    "\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax.set_xlabel(\"Dog Classification Algorithm\")\n",
    "\n",
    "ax.set_ylabel(\"Frequency\") \n",
    "\n",
    "g = sns.FacetGrid(data=merge_data1)\n",
    "\n",
    "g.map(sns.distplot,'p1_conf' ,color='blue',\n",
    "      kde=True, bins=15, ax=ax)\n",
    "\n",
    "g.map(sns.distplot,'p2_conf', color='purple',\n",
    "      kde=True, bins=15, ax=ax)\n",
    "\n",
    "g.map(sns.distplot,'p3_conf', color='pink',\n",
    "      kde=True, bins=15, ax=ax)\n",
    "\n",
    "ax.legend(title='algoritm')\n",
    "plt.close(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(131)\n",
    "\n",
    "merge_data1['p1'].value_counts(normalize=True).head(10).plot.bar(figsize=(24,6), fontsize = 15.0)\n",
    "plt.title('p1 - Algorithm Output', fontweight=\"bold\", fontsize = 22.0)\n",
    "plt.ylabel('Count %', fontsize = 20.0)\n",
    "\n",
    "plt.subplot(132)\n",
    "merge_data1['p2'].value_counts(normalize=True).head(10).plot.bar(figsize=(24,6), fontsize = 15.0)\n",
    "plt.title('p2 -Algorithm Output', fontweight=\"bold\", fontsize = 22.0)\n",
    "plt.ylabel('Count %', fontsize = 20.0)\n",
    "\n",
    "plt.subplot(133)\n",
    "merge_data1['p3'].value_counts(normalize=True).head(10).plot.bar(figsize=(24,6), fontsize = 15.0)\n",
    "plt.title('p3 - Algorithm Output', fontweight=\"bold\", fontsize = 22.0)\n",
    "plt.ylabel('Count %', fontsize = 20.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Result :\n",
    "- Reduce the result into one table for further Expolratory Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "base=merge_data1[['tweet_id','favorite_count','retweet_count']]\n",
    "feature=[base]+[melt1,melt2]\n",
    "abt=reduce(lambda left,right: pd.merge(left,right,on=['tweet_id']),[melt1,melt2])\n",
    "abt.tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abt.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abt.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abt['rating']=abt['rating'].fillna(0)\n",
    "abt.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abt[\"dog_stage\"].fillna(\"No_stages\",inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "abt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "abt[['p1_conf','p2_conf','p3_conf']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abt[abt.dog_stage!='None'].dog_stage.value_counts().plot.pie(subplots=True, title=\"Dog Stages\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store abt csv as twitter_archive_master "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "abt.to_csv('twitter_archive_master.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    " - Data wrangling is process to clean the dataand to provide structure and enhancing code readability \n",
    " - Scrape data (rating ,urls, image data , retweet and favorite_counts) from various sources  in different format (csv,json,jpg)\n",
    " - Data wrangling issues:quality issues and tidiness issues  \n",
    " - Fixing timestamp(string) datatype,remove unnecessary html tags ,missing data .\n",
    " - Melt() is used to combine the data.\n",
    " - Join melted DataFrames into one table using the merge() function.\n",
    " - To reduce this result into an analytical base table using reduce function()."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
